{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e96ab50-b396-4e48-81cc-160058738c4e",
      "metadata": {
        "id": "5e96ab50-b396-4e48-81cc-160058738c4e"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5ac5964c-acfa-4912-92af-7946c6fa7398",
      "metadata": {
        "id": "5ac5964c-acfa-4912-92af-7946c6fa7398"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from zip import unzip_archive\n",
        "from fedot_util import run_example\n",
        "from llm_util import run_model_multicall, process_model_responses\n",
        "from web_api import WebAssistant\n",
        "from data import Dataset\n",
        "import prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2ac346",
      "metadata": {},
      "source": [
        "# Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a1cabc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name = [\n",
        "    'titanic', \n",
        "    'credit-g'\n",
        "][0]\n",
        "dataset_path = os.sep.join(['datasets', dataset_name])\n",
        "\n",
        "# zip_filename = f\"{dataset_path}.zip\"\n",
        "# os.makedirs(dataset_path, exist_ok=True)\n",
        "# unzip_archive(zip_filename, dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "O3IlOI4PeVXT",
      "metadata": {
        "id": "O3IlOI4PeVXT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assume we have a dataset.\n",
            "The dataset contains the following splits:\n",
            " \n",
            "The gender_submission split stored in file \"gender_submission.csv\" contains following columns: ['PassengerId', 'Survived']. It is described as None\n",
            "The test split stored in file \"test.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
            "The train split stored in file \"train.csv\" contains following columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
            "\n",
            "name: None \n",
            "description: None \n",
            "goal: None \n",
            "train_split_name: None \n",
            "splits:\n",
            "\n",
            "name: gender_submission \n",
            "path: datasets\\titanic\\gender_submission.csv \n",
            "description: None\n",
            "\n",
            "name: test \n",
            "path: datasets\\titanic\\test.csv \n",
            "description: None\n",
            "\n",
            "name: train \n",
            "path: datasets\\titanic\\train.csv \n",
            "description: None\n"
          ]
        }
      ],
      "source": [
        "dataset = Dataset.load_from_path(dataset_path)\n",
        "dataset_description = dataset.get_description()\n",
        "dataset_metadata_description = dataset.get_metadata_description()\n",
        "\n",
        "print(dataset_description)\n",
        "print()\n",
        "print(dataset_metadata_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b1a48fba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open('datasets/big_descriptions.json', 'r') as json_file:\n",
        "    dataset_big_descriptions = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a52f6174",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'titanic': 'The sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\nIn this competition, you’ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv.\\n\\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”.\\n\\nThe test.csv dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes.\\n\\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\\n\\nCheck out the “Data” tab to explore the datasets even further. Once you feel you’ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n    \\n    Dataset Description\\nOverview\\nThe data has been split into two groups:\\n\\ntraining set (train.csv)\\ntest set (test.csv)\\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\\n\\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\\n\\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\\n    ',\n",
              " 'credit-g': 'German Credit dataset\\nThis dataset classifies people described by a set of attributes as good or bad credit risks.\\n\\nThis dataset comes with a cost matrix:\\n\\nGood  Bad (predicted)  \\nGood   0    1   (actual)  \\nBad    5    0  \\nIt is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\\n\\nAttribute description\\nStatus of existing checking account, in Deutsche Mark.\\nDuration in months\\nCredit history (credits taken, paid back duly, delays, critical accounts)\\nPurpose of the credit (car, television,...)\\nCredit amount\\nStatus of savings account/bonds, in Deutsche Mark.\\nPresent employment, in number of years.\\nInstallment rate in percentage of disposable income\\nPersonal status (married, single,...) and sex\\nOther debtors / guarantors\\nPresent residence since X years\\nProperty (e.g. real estate)\\nAge in years\\nOther installment plans (banks, stores)\\nHousing (rent, own,...)\\nNumber of existing credits at this bank\\nJob\\nNumber of people being liable to provide maintenance for\\nTelephone (yes,no)\\nForeign worker (yes,no)\\n    ',\n",
              " 'playground-series-s3e23': \"Welcome to the 2023 edition of Kaggle's Playground Series!\\nThank you to everyone who participated in and contributed to Season 3 Playground Series so far!\\n\\nWith the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in October every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\\n\\nYour Goal: Predict defects in C programs given various various attributes about the code.\\n\\nDataset Description\\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\\n\\nFiles\\ntrain.csv - the training dataset; defects is the binary target, which is treated as a boolean (False=0, True=1)\\ntest.csv - the test dataset; your objective is to predict the probability of positive defects (i.e., defects=True)\\nsample_submission.csv - a sample submission file in the correct format\\n\",\n",
              " 'playground-series-s4e3': 'Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting an approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.\\n\\nYour Goal: Predict the probability of various defects on steel plates. Good luck!\\n\\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\\n\\nFiles\\ntrain.csv - the training dataset; there are 7 binary targets: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults\\ntest.csv - the test dataset; your objective is to predict the probability of each of the 7 binary targets\\nsample_submission.csv - a sample submission file in the correct format\\n',\n",
              " 'steel-dataset': 'Content\\nThis company produces several types of coils, steel plates, and iron plates. The information on electricity consumption is held in a cloud-based system. The information on energy consumption of the industry is stored on the website of the Korea Electric Power Corporation (pccs.kepco.go.kr), and the perspectives on daily, monthly, and annual data are calculated and shown.\\n\\nAttribute Information:\\nDate Continuous-time data taken on the first of the month\\nUsage_kWh Industry Energy Consumption Continuous kWh\\nLagging Current reactive power Continuous kVarh\\nLeading Current reactive power Continuous kVarh\\nCO2 Continuous ppm\\nNSM Number of Seconds from midnight Continuous S\\nWeek status Categorical (Weekend (0) or a Weekday(1))\\nDay of week Categorical Sunday, Monday : Saturday\\nLoad Type Categorical Light Load, Medium Load, Maximum Load\\n\\nAcknowledgements\\nThis dataset is sourced from the UCI Machine Learning Repository\\n'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_big_descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4c1510",
      "metadata": {},
      "source": [
        "# Выбор модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e298e859",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_type = [\"8b\", \"70b\"][0]\n",
        "url = 'http://10.32.2.2:8672/v1/chat/completions'\n",
        "\n",
        "if model_type == \"70b\":\n",
        "    url = 'http://10.32.15.21:6672/generate'\n",
        "\n",
        "model = WebAssistant(url, model_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34849793",
      "metadata": {},
      "source": [
        "# Уточнение данных о датасете"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8e8e9cd7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataset_name': 'Titanic Survival Dataset', 'train_split': 'train'}\n"
          ]
        }
      ],
      "source": [
        "# 1: Название датасета и определение тренировочного сплита\n",
        "task_prompts = {\n",
        "    \"dataset_name\": {\n",
        "        \"system\": dataset_big_descriptions[dataset_name],\n",
        "        \"task\": prompts.dataset_name_prompt,\n",
        "        \"context\": None,\n",
        "    },\n",
        "    \"train_split\": {\n",
        "        \"system\": dataset_big_descriptions[dataset_name],\n",
        "        \"task\": prompts.train_split_definition_prompt,\n",
        "        \"context\": dataset.get_description(),\n",
        "    }\n",
        "}\n",
        "\n",
        "responses = run_model_multicall(\n",
        "    model, task_prompts\n",
        ")\n",
        "operations = {\n",
        "    \"train_split\": lambda x : x.split(\".\")[0]\n",
        "}\n",
        "responses = process_model_responses(responses, operations)\n",
        "pprint(responses)\n",
        "\n",
        "dataset.name = responses[\"dataset_name\"]\n",
        "dataset.train_split_name = responses[\"train_split\"] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataset_description': 'Here is a short description of the Titanic Survival '\n",
            "                        'Dataset:\\n'\n",
            "                        '\\n'\n",
            "                        'The Titanic Survival Dataset contains passenger '\n",
            "                        \"information from the RMS Titanic's ill-fated maiden \"\n",
            "                        'voyage in 1912. The dataset consists of three splits: '\n",
            "                        'gender_submission, test, and train. The '\n",
            "                        'gender_submission split contains only two columns: '\n",
            "                        'PassengerId and Survived. The test and train splits '\n",
            "                        'contain additional columns such as Pclass (social '\n",
            "                        'class), Name, Sex, Age, SibSp (number of '\n",
            "                        'siblings/spouses on board), Parch (number of '\n",
            "                        'parents/children on board), Ticket, Fare, Cabin, and '\n",
            "                        'Embarked (port of embarkation). The goal is to use '\n",
            "                        'the patterns in the train data to predict whether '\n",
            "                        'passengers survived or not based on their '\n",
            "                        'demographics and other features.',\n",
            " 'dataset_goal': 'Task: Predict the survival of passengers on the Titanic '\n",
            "                 'based on their demographic and socioeconomic '\n",
            "                 'characteristics.\\n'\n",
            "                 '\\n'\n",
            "                 'Target column: Survived.'}\n"
          ]
        }
      ],
      "source": [
        "# 2: Цель всей задачи\n",
        "\n",
        "task_prompts = {\n",
        "    \"dataset_description\": {\n",
        "        \"system\": dataset_big_descriptions[dataset_name],\n",
        "        \"task\": prompts.dataset_description_prompt,\n",
        "        \"context\": dataset.get_description(),\n",
        "    },\n",
        "    \"dataset_goal\": {\n",
        "        \"system\": dataset_big_descriptions[dataset_name],\n",
        "        \"task\": prompts.dataset_goal_prompt,\n",
        "        \"context\": dataset.get_description(),\n",
        "    },\n",
        "}\n",
        "\n",
        "responses = run_model_multicall(\n",
        "    model, task_prompts\n",
        ")\n",
        "pprint(responses)\n",
        "\n",
        "dataset.description = responses[\"dataset_description\"]\n",
        "dataset.goal = responses[\"dataset_goal\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "06ec2375",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'categorical_columns': ['Pclass', 'Sex', 'Embarked'],\n",
            " 'target_column': \"'Survived'\",\n",
            " 'task_type': 'classification'}\n"
          ]
        }
      ],
      "source": [
        "# 2: Категориальные столбцы, таргет-столбец, тип задачи\n",
        "\n",
        "task_prompts = {\n",
        "    \"categorical_columns\": {\n",
        "        \"system\": dataset_description,\n",
        "        \"task\": prompts.categorical_definition_prompt,\n",
        "        \"context\": prompts.categorical_definition_context,\n",
        "    },\n",
        "    \"target_column\": {\n",
        "        \"system\": dataset_description,\n",
        "        \"task\": prompts.target_definition_prompt,\n",
        "        \"context\": None,\n",
        "    },\n",
        "    \"task_type\": {\n",
        "        \"system\": dataset_description,\n",
        "        \"task\": prompts.task_definition_prompt,\n",
        "        \"context\": None,\n",
        "    }\n",
        "}\n",
        "\n",
        "#Выбор модели\n",
        "\n",
        "model_type = [\"8b\", \"70b\"][0]\n",
        "url = 'http://10.32.2.2:8672/v1/chat/completions'\n",
        "\n",
        "if model_type == \"70b\":\n",
        "    url = 'http://10.32.15.21:6672/generate'\n",
        "\n",
        "model = WebAssistant(url, model_type)\n",
        "responses = run_model_multicall(\n",
        "    model, task_prompts\n",
        ")\n",
        "\n",
        "pattern = r'[\\'\\\"“”‘’`´]'\n",
        "operations = {\n",
        "    \"categorical_columns\": lambda x : x.split(\"\\n\"),\n",
        "    \"task_type\": lambda x : re.sub(pattern, '', x.lower())\n",
        "}\n",
        "responses = process_model_responses(responses, operations)\n",
        "pprint(responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8382401d",
      "metadata": {},
      "source": [
        "# Запуск фреймворка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aaJnOwCvr_wR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "aaJnOwCvr_wR",
        "outputId": "9eec85e6-9062-4ef6-b018-bd8d7a8eb587"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generations:   0%|          | 0/10000 [02:16<?, ?gen/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'roc_auc': 0.941, 'accuracy': 0.828}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if dataset_path == 'titanic':\n",
        "    test_df = dataset_metadata[\"splits\"][\"test_X\"].merge(dataset_metadata[\"splits\"][\"test_y\"],\n",
        "                                                         on='PassengerId', how='inner')\n",
        "else:\n",
        "    test_df = dataset_metadata[\"splits\"][\"test\"]\n",
        "\n",
        "train_df = dataset_metadata[\"splits\"][\"train\"]\n",
        "\n",
        "prediction = run_example(train_df = train_df, test_df = test_df,\n",
        "                          dataset_metadata = dataset_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2f61d093",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1]], dtype=int64)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e45d4f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "result_df = pd.DataFrame(prediction, columns=[dataset_metadata[\"target_column\"]])\n",
        "\n",
        "result_df.to_csv(f\"{dataset_path}/predictions.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5e96ab50-b396-4e48-81cc-160058738c4e"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
