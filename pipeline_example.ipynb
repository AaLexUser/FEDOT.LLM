{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e96ab50-b396-4e48-81cc-160058738c4e",
   "metadata": {
    "id": "5e96ab50-b396-4e48-81cc-160058738c4e"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac5964c-acfa-4912-92af-7946c6fa7398",
   "metadata": {
    "id": "5ac5964c-acfa-4912-92af-7946c6fa7398"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from zip import unzip_archive\n",
    "from fedot_util import run_example\n",
    "from llm_util import run_model_multicall, process_model_responses\n",
    "from web_api import WebAssistant\n",
    "from data import Dataset\n",
    "import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ac346",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1cabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = [\n",
    "    'titanic', \n",
    "    'credit-g'\n",
    "][0]\n",
    "dataset_path = os.sep.join(['datasets', dataset_name])\n",
    "\n",
    "# zip_filename = f\"{dataset_path}.zip\"\n",
    "# os.makedirs(dataset_path, exist_ok=True)\n",
    "# unzip_archive(zip_filename, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "O3IlOI4PeVXT",
   "metadata": {
    "id": "O3IlOI4PeVXT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we have a dataset\n",
      "The dataset contains the following splits:\n",
      " \n",
      "The test_merged split stored in file \"test_merged.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Survived']. It is described as None\n",
      "The test split stored in file \"test.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The predictions split stored in file \"predictions.csv\" contains following columns: ['Unnamed: 0', 'Survived']. It is described as None\n",
      "The train split stored in file \"train.csv\" contains following columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The gender_submission split stored in file \"gender_submission.csv\" contains following columns: ['PassengerId', 'Survived']. It is described as None\n",
      "\n",
      "name: None \n",
      "description: None \n",
      "goal: None \n",
      "train_split_name: None \n",
      "splits:\n",
      "\n",
      "name: test_merged \n",
      "path: datasets/titanic/test_merged.csv \n",
      "description: None\n",
      "\n",
      "name: test \n",
      "path: datasets/titanic/test.csv \n",
      "description: None\n",
      "\n",
      "name: predictions \n",
      "path: datasets/titanic/predictions.csv \n",
      "description: None\n",
      "\n",
      "name: train \n",
      "path: datasets/titanic/train.csv \n",
      "description: None\n",
      "\n",
      "name: gender_submission \n",
      "path: datasets/titanic/gender_submission.csv \n",
      "description: None\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.load_from_path(dataset_path)\n",
    "dataset_description = dataset.get_description()\n",
    "dataset_metadata_description = dataset.get_metadata_description()\n",
    "\n",
    "print(dataset_description)\n",
    "print()\n",
    "print(dataset_metadata_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a48fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('datasets/big_descriptions.json', 'r') as json_file:\n",
    "    dataset_big_descriptions = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52f6174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titanic': 'The sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\nIn this competition, you’ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv.\\n\\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”.\\n\\nThe test.csv dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes.\\n\\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\\n\\nCheck out the “Data” tab to explore the datasets even further. Once you feel you’ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n    \\n    Dataset Description\\nOverview\\nThe data has been split into two groups:\\n\\ntraining set (train.csv)\\ntest set (test.csv)\\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\\n\\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\\n\\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\\n    ',\n",
       " 'credit-g': 'German Credit dataset\\nThis dataset classifies people described by a set of attributes as good or bad credit risks.\\n\\nThis dataset comes with a cost matrix:\\n\\nGood  Bad (predicted)  \\nGood   0    1   (actual)  \\nBad    5    0  \\nIt is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\\n\\nAttribute description\\nStatus of existing checking account, in Deutsche Mark.\\nDuration in months\\nCredit history (credits taken, paid back duly, delays, critical accounts)\\nPurpose of the credit (car, television,...)\\nCredit amount\\nStatus of savings account/bonds, in Deutsche Mark.\\nPresent employment, in number of years.\\nInstallment rate in percentage of disposable income\\nPersonal status (married, single,...) and sex\\nOther debtors / guarantors\\nPresent residence since X years\\nProperty (e.g. real estate)\\nAge in years\\nOther installment plans (banks, stores)\\nHousing (rent, own,...)\\nNumber of existing credits at this bank\\nJob\\nNumber of people being liable to provide maintenance for\\nTelephone (yes,no)\\nForeign worker (yes,no)\\n    ',\n",
       " 'playground-series-s3e23': \"Welcome to the 2023 edition of Kaggle's Playground Series!\\nThank you to everyone who participated in and contributed to Season 3 Playground Series so far!\\n\\nWith the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in October every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\\n\\nYour Goal: Predict defects in C programs given various various attributes about the code.\\n\\nDataset Description\\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\\n\\nFiles\\ntrain.csv - the training dataset; defects is the binary target, which is treated as a boolean (False=0, True=1)\\ntest.csv - the test dataset; your objective is to predict the probability of positive defects (i.e., defects=True)\\nsample_submission.csv - a sample submission file in the correct format\\n\",\n",
       " 'playground-series-s4e3': 'Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting an approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.\\n\\nYour Goal: Predict the probability of various defects on steel plates. Good luck!\\n\\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\\n\\nFiles\\ntrain.csv - the training dataset; there are 7 binary targets: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults\\ntest.csv - the test dataset; your objective is to predict the probability of each of the 7 binary targets\\nsample_submission.csv - a sample submission file in the correct format\\n',\n",
       " 'steel-dataset': 'Content\\nThis company produces several types of coils, steel plates, and iron plates. The information on electricity consumption is held in a cloud-based system. The information on energy consumption of the industry is stored on the website of the Korea Electric Power Corporation (pccs.kepco.go.kr), and the perspectives on daily, monthly, and annual data are calculated and shown.\\n\\nAttribute Information:\\nDate Continuous-time data taken on the first of the month\\nUsage_kWh Industry Energy Consumption Continuous kWh\\nLagging Current reactive power Continuous kVarh\\nLeading Current reactive power Continuous kVarh\\nCO2 Continuous ppm\\nNSM Number of Seconds from midnight Continuous S\\nWeek status Categorical (Weekend (0) or a Weekday(1))\\nDay of week Categorical Sunday, Monday : Saturday\\nLoad Type Categorical Light Load, Medium Load, Maximum Load\\n\\nAcknowledgements\\nThis dataset is sourced from the UCI Machine Learning Repository\\n'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_big_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c1510",
   "metadata": {},
   "source": [
    "# Выбор модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e298e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = [\"8b\", \"70b\"][0]\n",
    "url = 'http://10.32.2.2:8672/v1/chat/completions'\n",
    "\n",
    "if model_type == \"70b\":\n",
    "    url = 'http://10.32.15.21:6672/generate'\n",
    "\n",
    "model = WebAssistant(url, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34849793",
   "metadata": {},
   "source": [
    "# Уточнение данных о датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e8e9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'Titanic Survival Dataset', 'train_split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# 1: Название датасета и определение тренировочного сплита\n",
    "task_prompts = {\n",
    "    \"dataset_name\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_name_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"train_split\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.train_split_definition_prompt,\n",
    "        \"context\": dataset.get_description(),\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = run_model_multicall(\n",
    "    model, task_prompts\n",
    ")\n",
    "operations = {\n",
    "    \"train_split\": lambda x : x.split(\".\")[0]\n",
    "}\n",
    "responses = process_model_responses(responses, operations)\n",
    "pprint(responses)\n",
    "\n",
    "dataset.name = responses[\"dataset_name\"]\n",
    "dataset.train_split_name = responses[\"train_split\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fca38e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_description': 'Here is a short description of the dataset:\\n'\n",
      "                        '\\n'\n",
      "                        'The Titanic Survival Dataset contains information on '\n",
      "                        'passengers who boarded the RMS Titanic during its '\n",
      "                        'ill-fated maiden voyage in 1912. The dataset is split '\n",
      "                        'into six files: test_merged.csv, test.csv, '\n",
      "                        'predictions.csv, train.csv, and '\n",
      "                        'gender_submission.csv. Each file contains various '\n",
      "                        'attributes such as passenger ID, class, name, sex, '\n",
      "                        'age, family size, ticket number, fare, cabin '\n",
      "                        'information, and embarked port. The goal is to '\n",
      "                        'predict whether each passenger survived the sinking '\n",
      "                        'of the Titanic based on these features.',\n",
      " 'dataset_goal': 'Task: Predict the likelihood of survival for passengers on '\n",
      "                 'the Titanic based on their demographic information (target '\n",
      "                 'column: Survived).'}\n"
     ]
    }
   ],
   "source": [
    "# 2: Цель всей задачи\n",
    "\n",
    "task_prompts = {\n",
    "    \"dataset_description\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_description_prompt,\n",
    "        \"context\": dataset.get_description(),\n",
    "    },\n",
    "    \"dataset_goal\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_goal_prompt,\n",
    "        \"context\": dataset.get_description(),\n",
    "    },\n",
    "}\n",
    "\n",
    "responses = run_model_multicall(\n",
    "    model, task_prompts\n",
    ")\n",
    "pprint(responses)\n",
    "\n",
    "dataset.description = responses[\"dataset_description\"]\n",
    "dataset.goal = responses[\"dataset_goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ec2375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_columns': ['Pclass', 'Sex', 'Embarked', 'SibSp', 'Parch'],\n",
      " 'target_column': 'Survived',\n",
      " 'task_type': 'classification'}\n"
     ]
    }
   ],
   "source": [
    "# 2: Категориальные столбцы, таргет-столбец, тип задачи\n",
    "\n",
    "task_prompts = {\n",
    "    \"categorical_columns\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.categorical_definition_prompt,\n",
    "        \"context\": prompts.categorical_definition_context,\n",
    "    },\n",
    "    \"target_column\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.target_definition_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"task_type\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.task_definition_prompt,\n",
    "        \"context\": None,\n",
    "    }\n",
    "}\n",
    "\n",
    "#Выбор модели\n",
    "\n",
    "model_type = [\"8b\", \"70b\"][0]\n",
    "url = 'http://10.32.2.2:8672/v1/chat/completions'\n",
    "\n",
    "if model_type == \"70b\":\n",
    "    url = 'http://10.32.15.21:6672/generate'\n",
    "\n",
    "model = WebAssistant(url, model_type)\n",
    "responses = run_model_multicall(\n",
    "    model, task_prompts\n",
    ")\n",
    "\n",
    "pattern = r'[\\'\\\"“”‘’`´]'\n",
    "operations = {\n",
    "    \"categorical_columns\": lambda x : x.split(\"\\n\"),\n",
    "    \"target_column\" :  lambda x : re.sub(pattern, '', x),\n",
    "    \"task_type\": lambda x : re.sub(pattern, '', x.lower())\n",
    "}\n",
    "responses = process_model_responses(responses, operations)\n",
    "pprint(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8382401d",
   "metadata": {},
   "source": [
    "# Запуск фреймворка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd73bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test_merged\n"
     ]
    }
   ],
   "source": [
    "print(dataset.splits[3].name)\n",
    "print(dataset.splits[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab960411-6fb6-4332-8165-478f01e11967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.splits[3].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaJnOwCvr_wR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "aaJnOwCvr_wR",
    "outputId": "9eec85e6-9062-4ef6-b018-bd8d7a8eb587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24 12:26:05,790 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generations:   0%|          | 0/10000 [00:00<?, ?gen/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24 12:26:12,193 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:12,193 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:12,193 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,555 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,565 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,577 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,612 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,628 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,649 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,661 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,674 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 12:26:33,788 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generations:   0%|          | 0/10000 [18:13<?, ?gen/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc': 0.941, 'accuracy': 0.828}\n"
     ]
    }
   ],
   "source": [
    "prediction = run_example(train_df=dataset.splits[3].data, test_df=dataset.splits[0].data, problem=responses['task_type'], target=responses['target_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f61d093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e45d4f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(prediction, columns\u001b[38;5;241m=\u001b[39m[\u001b[43mdataset_metadata\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      3\u001b[0m result_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_metadata' is not defined"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(prediction, columns=[dataset_metadata[\"target_column\"]])\n",
    "\n",
    "result_df.to_csv(f\"{dataset_path}/predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5e96ab50-b396-4e48-81cc-160058738c4e"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
