{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e96ab50-b396-4e48-81cc-160058738c4e",
   "metadata": {
    "id": "5e96ab50-b396-4e48-81cc-160058738c4e"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac5964c-acfa-4912-92af-7946c6fa7398",
   "metadata": {
    "id": "5ac5964c-acfa-4912-92af-7946c6fa7398"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fedot_llm.data.zip import unzip_archive\n",
    "from fedot_llm.fedot_util import run_example\n",
    "from fedot_llm.language_models.actions import ModelAction\n",
    "from fedot_llm.language_models.llms import CustomWebLLM\n",
    "from fedot_llm.data.data import Dataset\n",
    "import fedot_llm.language_models.prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ac346",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1cabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = [\n",
    "    'titanic', \n",
    "    'credit-g'\n",
    "][0]\n",
    "dataset_path = os.sep.join(['..', 'datasets', dataset_name])\n",
    "\n",
    "# zip_filename = f\"{dataset_path}.zip\"\n",
    "# os.makedirs(dataset_path, exist_ok=True)\n",
    "# unzip_archive(zip_filename, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "O3IlOI4PeVXT",
   "metadata": {
    "id": "O3IlOI4PeVXT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we have a dataset\n",
      "The dataset contains the following splits:\n",
      " \n",
      "The test_merged split stored in file \"test_merged.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Survived']. It is described as None\n",
      "The test split stored in file \"test.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The predictions split stored in file \"predictions.csv\" contains following columns: ['Unnamed: 0', 'Survived']. It is described as None\n",
      "The train split stored in file \"train.csv\" contains following columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The gender_submission split stored in file \"gender_submission.csv\" contains following columns: ['PassengerId', 'Survived']. It is described as None\n",
      "\n",
      "name: None \n",
      "description: None \n",
      "goal: None \n",
      "train_split_name: None \n",
      "splits:\n",
      "\n",
      "name: test_merged \n",
      "path: ../datasets/titanic/test_merged.csv \n",
      "description: None\n",
      "\n",
      "name: test \n",
      "path: ../datasets/titanic/test.csv \n",
      "description: None\n",
      "\n",
      "name: predictions \n",
      "path: ../datasets/titanic/predictions.csv \n",
      "description: None\n",
      "\n",
      "name: train \n",
      "path: ../datasets/titanic/train.csv \n",
      "description: None\n",
      "\n",
      "name: gender_submission \n",
      "path: ../datasets/titanic/gender_submission.csv \n",
      "description: None\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.load_from_path(dataset_path)\n",
    "dataset_description = dataset.get_description()\n",
    "dataset_metadata_description = dataset.get_metadata_description()\n",
    "\n",
    "print(dataset_description)\n",
    "print()\n",
    "print(dataset_metadata_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a48fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../datasets/big_descriptions.json', 'r') as json_file:\n",
    "    dataset_big_descriptions = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52f6174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titanic': 'The sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\nIn this competition, you’ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv.\\n\\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”.\\n\\nThe test.csv dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes.\\n\\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\\n\\nCheck out the “Data” tab to explore the datasets even further. Once you feel you’ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n    \\n    Dataset Description\\nOverview\\nThe data has been split into two groups:\\n\\ntraining set (train.csv)\\ntest set (test.csv)\\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\\n\\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\\n\\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\\n    ',\n",
       " 'credit-g': 'German Credit dataset\\nThis dataset classifies people described by a set of attributes as good or bad credit risks.\\n\\nThis dataset comes with a cost matrix:\\n\\nGood  Bad (predicted)  \\nGood   0    1   (actual)  \\nBad    5    0  \\nIt is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\\n\\nAttribute description\\nStatus of existing checking account, in Deutsche Mark.\\nDuration in months\\nCredit history (credits taken, paid back duly, delays, critical accounts)\\nPurpose of the credit (car, television,...)\\nCredit amount\\nStatus of savings account/bonds, in Deutsche Mark.\\nPresent employment, in number of years.\\nInstallment rate in percentage of disposable income\\nPersonal status (married, single,...) and sex\\nOther debtors / guarantors\\nPresent residence since X years\\nProperty (e.g. real estate)\\nAge in years\\nOther installment plans (banks, stores)\\nHousing (rent, own,...)\\nNumber of existing credits at this bank\\nJob\\nNumber of people being liable to provide maintenance for\\nTelephone (yes,no)\\nForeign worker (yes,no)\\n    ',\n",
       " 'playground-series-s3e23': \"Welcome to the 2023 edition of Kaggle's Playground Series!\\nThank you to everyone who participated in and contributed to Season 3 Playground Series so far!\\n\\nWith the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in October every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\\n\\nYour Goal: Predict defects in C programs given various various attributes about the code.\\n\\nDataset Description\\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\\n\\nFiles\\ntrain.csv - the training dataset; defects is the binary target, which is treated as a boolean (False=0, True=1)\\ntest.csv - the test dataset; your objective is to predict the probability of positive defects (i.e., defects=True)\\nsample_submission.csv - a sample submission file in the correct format\\n\",\n",
       " 'playground-series-s4e3': 'Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting an approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.\\n\\nYour Goal: Predict the probability of various defects on steel plates. Good luck!\\n\\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\\n\\nFiles\\ntrain.csv - the training dataset; there are 7 binary targets: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults\\ntest.csv - the test dataset; your objective is to predict the probability of each of the 7 binary targets\\nsample_submission.csv - a sample submission file in the correct format\\n',\n",
       " 'steel-dataset': 'Content\\nThis company produces several types of coils, steel plates, and iron plates. The information on electricity consumption is held in a cloud-based system. The information on energy consumption of the industry is stored on the website of the Korea Electric Power Corporation (pccs.kepco.go.kr), and the perspectives on daily, monthly, and annual data are calculated and shown.\\n\\nAttribute Information:\\nDate Continuous-time data taken on the first of the month\\nUsage_kWh Industry Energy Consumption Continuous kWh\\nLagging Current reactive power Continuous kVarh\\nLeading Current reactive power Continuous kVarh\\nCO2 Continuous ppm\\nNSM Number of Seconds from midnight Continuous S\\nWeek status Categorical (Weekend (0) or a Weekday(1))\\nDay of week Categorical Sunday, Monday : Saturday\\nLoad Type Categorical Light Load, Medium Load, Maximum Load\\n\\nAcknowledgements\\nThis dataset is sourced from the UCI Machine Learning Repository\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_big_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c1510",
   "metadata": {},
   "source": [
    "# Выбор модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e298e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = [\"8b\", \"70b\"][0]\n",
    "url = 'http://10.32.2.2:8672/v1/chat/completions'\n",
    "\n",
    "if model_type == \"70b\":\n",
    "    url = 'http://10.32.15.21:6672/generate'\n",
    "\n",
    "model = CustomWebLLM(url, model=model_type, timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34849793",
   "metadata": {},
   "source": [
    "# Уточнение данных о датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8e9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'Titanic Survivorship Prediction Dataset',\n",
      " 'train_split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "from fedot_llm.language_models import prompts\n",
    "\n",
    "action = ModelAction(model=model)\n",
    "# 1: Название датасета и определение тренировочного сплита\n",
    "task_prompts = {\n",
    "    \"dataset_name\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_name_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"train_split\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.train_split_definition_prompt,\n",
    "        \"context\": dataset.get_description(),\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "operations = {\n",
    "    \"train_split\": lambda x : x.split(\".\")[0]\n",
    "}\n",
    "responses = action.process_model_responses(responses, operations)\n",
    "pprint(responses)\n",
    "\n",
    "dataset.name = responses[\"dataset_name\"]\n",
    "dataset.train_split_name = responses[\"train_split\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e96502e-2cfe-4c84-8d6d-fa40018818e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = list(filter(lambda split: split.name == dataset.train_split_name, dataset.splits))[0]\n",
    "train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f259285b-b1c4-4737-ae2a-e05c5daf70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_descriptions = action.generate_all_column_description(split=train, dataset=dataset)\n",
    "train.set_column_descriptions(column_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc9834d-9964-43ce-b0f5-c632ff0e80f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PassengerId': 'sequential unique identifier for each passenger',\n",
       " 'Survived': 'a binary value indicating whether a passenger survived (1) or not (0)',\n",
       " 'Pclass': 'passenger class, with 1 being the first class and 3 being the third class',\n",
       " 'Name': 'the names of passengers on the titanic, including titles and suffixes',\n",
       " 'Sex': 'binary classification indicating the gender of a passenger: male or female.',\n",
       " 'Age': 'numerical values representing ages of passengers, ranging from 0.75 to 74.',\n",
       " 'SibSp': 'number of siblings and spouses on board the ship',\n",
       " 'Parch': 'number of parents and/or children accompanied on the voyage',\n",
       " 'Ticket': 'unique identifiers for passengers and crew, including ticket numbers and cabin information',\n",
       " 'Fare': 'continuous value representing the fare paid by passengers in british pounds, ranging from £6.24 to £263',\n",
       " 'Cabin': \"a column containing cabin numbers or combinations of numbers, with some values missing (represented by 'nan').\",\n",
       " 'Embarked': \"the embarkation port of passengers, represented by 's' for southampton, 'c' for cherbourg and 'q' for queenstown\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca38e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_description': 'Here is a short description of the dataset:\\n'\n",
      "                        '\\n'\n",
      "                        'The Titanic dataset contains information about 2224 '\n",
      "                        'passengers who boarded the ill-fated RMS Titanic on '\n",
      "                        'its maiden voyage in 1912. The dataset includes '\n",
      "                        'columns for passenger identification, survival '\n",
      "                        'status, class, name, gender, age, family '\n",
      "                        'relationships, ticket information, fare paid, cabin '\n",
      "                        'number, and embarkation port. With a mix of '\n",
      "                        'categorical and numerical variables, this dataset '\n",
      "                        'provides insights into the demographics and '\n",
      "                        'characteristics of the passengers on board, allowing '\n",
      "                        'for analysis and prediction of who survived the '\n",
      "                        'tragic event.',\n",
      " 'dataset_goal': 'Formulate a predictive model that predicts whether a '\n",
      "                 'passenger survived or not based on the available features. '\n",
      "                 'The target column is \"Survived\".'}\n"
     ]
    }
   ],
   "source": [
    "# 2: Цель всей задачи\n",
    "\n",
    "task_prompts = {\n",
    "    \"dataset_description\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_description_prompt,\n",
    "        \"context\": dataset.get_description(),\n",
    "    },\n",
    "    \"dataset_goal\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_goal_prompt,\n",
    "        \"context\": dataset.get_description(),\n",
    "    },\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "pprint(responses)\n",
    "\n",
    "dataset.description = responses[\"dataset_description\"]\n",
    "dataset.goal = responses[\"dataset_goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06ec2375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_columns': ['Pclass', 'Sex', 'Embarked', 'SibSp', 'Parch'],\n",
      " 'target_column': 'Survived',\n",
      " 'task_type': 'classification'}\n"
     ]
    }
   ],
   "source": [
    "# 2: Категориальные столбцы, таргет-столбец, тип задачи\n",
    "\n",
    "task_prompts = {\n",
    "    \"categorical_columns\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.categorical_definition_prompt,\n",
    "        \"context\": prompts.categorical_definition_context,\n",
    "    },\n",
    "    \"target_column\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.target_definition_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"task_type\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.task_definition_prompt,\n",
    "        \"context\": None,\n",
    "    }\n",
    "}\n",
    "\n",
    "#Выбор модели\n",
    "\n",
    "model_type = [\"8b\", \"70b\"][0]\n",
    "url = 'http://10.32.2.2:8672/v1/chat/completions'\n",
    "\n",
    "if model_type == \"70b\":\n",
    "    url = 'http://10.32.15.21:6672/generate'\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "\n",
    "pattern = r'[\\'\\\"“”‘’`´]'\n",
    "operations = {\n",
    "    \"categorical_columns\": lambda x : x.split(\"\\n\"),\n",
    "    \"target_column\" :  lambda x : re.sub(pattern, '', x),\n",
    "    \"task_type\": lambda x : re.sub(pattern, '', x.lower())\n",
    "}\n",
    "responses = action.process_model_responses(responses, operations)\n",
    "pprint(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8382401d",
   "metadata": {},
   "source": [
    "# Запуск фреймворка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd73bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test_merged\n"
     ]
    }
   ],
   "source": [
    "print(dataset.splits[3].name)\n",
    "print(dataset.splits[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab960411-6fb6-4332-8165-478f01e11967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.splits[3].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaJnOwCvr_wR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "aaJnOwCvr_wR",
    "outputId": "9eec85e6-9062-4ef6-b018-bd8d7a8eb587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:53:46,959 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generations:   0%|          | 0/10000 [00:00<?, ?gen/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:53:54,219 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 15:53:54,219 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 15:53:54,219 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generations:   0%|          | 0/10000 [00:07<?, ?gen/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mrun_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtask_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/fedot_llm/fedot_util.py:21\u001b[0m, in \u001b[0;36mrun_example\u001b[0;34m(train_df, test_df, problem, target, cv_folds, metric, n_jobs, visualise, with_tuning, timeout, preset)\u001b[0m\n\u001b[1;32m     16\u001b[0m composer_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_history_dir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreset\u001b[39m\u001b[38;5;124m'\u001b[39m: preset}\n\u001b[1;32m     17\u001b[0m auto_model \u001b[38;5;241m=\u001b[39m Fedot(problem\u001b[38;5;241m=\u001b[39mproblem, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout, cv_folds \u001b[38;5;241m=\u001b[39m cv_folds,\n\u001b[1;32m     18\u001b[0m                    metric \u001b[38;5;241m=\u001b[39m metric, logging_level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mFATAL,\n\u001b[1;32m     19\u001b[0m                    with_tuning\u001b[38;5;241m=\u001b[39mwith_tuning, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomposer_params)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mauto_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m prediction \u001b[38;5;241m=\u001b[39m auto_model\u001b[38;5;241m.\u001b[39mpredict(features\u001b[38;5;241m=\u001b[39mtest_df)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualise:\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/fedot/api/main.py:176\u001b[0m, in \u001b[0;36mFedot.fit\u001b[0;34m(self, features, target, predefined_model)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_pipeline \u001b[38;5;241m=\u001b[39m PredefinedModel(predefined_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog,\n\u001b[1;32m    173\u001b[0m                                             use_input_preprocessing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    174\u001b[0m                                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_input_preprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_pipeline, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_models, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_composer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo models were found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/fedot/api/api_utils/api_composer.py:73\u001b[0m, in \u001b[0;36mApiComposer.obtain_model\u001b[0;34m(self, train_data)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39minit_params_for_composing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39mtimedelta_composing, multi_objective)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mmessage(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoML configured.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Parameters tuning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwith_tuning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Time limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Set of candidate models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavailable_operations\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m     best_pipeline, best_pipeline_candidates, gp_composer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_assumption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfitted_assumption\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_tuning:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m fedot_composer_timer\u001b[38;5;241m.\u001b[39mlaunch_tuning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomposing\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/fedot/api/api_utils/api_composer.py:137\u001b[0m, in \u001b[0;36mApiComposer.compose_pipeline\u001b[0;34m(self, train_data, initial_assumption, fitted_assumption)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mmessage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPipeline composition started.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_optimised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m best_pipelines \u001b[38;5;241m=\u001b[39m \u001b[43mgp_composer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m best_pipeline_candidates \u001b[38;5;241m=\u001b[39m gp_composer\u001b[38;5;241m.\u001b[39mbest_models\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_optimised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/fedot/core/composer/gp_composer/gp_composer.py:67\u001b[0m, in \u001b[0;36mGPComposer.compose_pipeline\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mset_evaluation_callback(objective_evaluator\u001b[38;5;241m.\u001b[39mevaluate_intermediate_metrics)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Finally, run optimization process\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m opt_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m best_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_opt_results_to_pipeline(opt_result)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGP composition finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/golem/core/optimisers/populational_optimizer.py:92\u001b[0m, in \u001b[0;36mPopulationalOptimizer.optimise\u001b[0;34m(self, objective)\u001b[0m\n\u001b[1;32m     88\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dispatcher\u001b[38;5;241m.\u001b[39mdispatch(objective, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progressbar \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_optimization():\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/golem/core/optimisers/genetic/gp_optimizer.py:67\u001b[0m, in \u001b[0;36mEvoGraphOptimizer._initial_population\u001b[0;34m(self, evaluator)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Initializes the initial population \"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Adding of initial assumptions to history as zero generation\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_population(\u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_individuals\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_assumptions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m pop_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_optimizer_params\u001b[38;5;241m.\u001b[39mpop_size\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_individuals) \u001b[38;5;241m<\u001b[39m pop_size:\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/golem/core/optimisers/genetic/evaluation.py:199\u001b[0m, in \u001b[0;36mBaseGraphEvaluationDispatcher.evaluate_with_cache\u001b[0;34m(self, population)\u001b[0m\n\u001b[1;32m    197\u001b[0m reversed_population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(population))\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_compute_cache(reversed_population)\n\u001b[0;32m--> 199\u001b[0m evaluated_population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreversed_population\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_eval_cache()\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m evaluated_population\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/golem/core/optimisers/genetic/evaluation.py:249\u001b[0m, in \u001b[0;36mMultiprocessingDispatcher.evaluate_population\u001b[0;34m(self, individuals)\u001b[0m\n\u001b[1;32m    247\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2*n_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m eval_func \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_single, logs_initializer\u001b[38;5;241m=\u001b[39mLog()\u001b[38;5;241m.\u001b[39mget_parameters())\n\u001b[0;32m--> 249\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_func\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindividuals_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m individuals_evaluated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_evaluation_results(individuals_to_evaluate, evaluation_results)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# If there were no successful evals then try once again getting at least one,\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# even if time limit was reached\u001b[39;00m\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Work/AutoML-LLM/AutoML-LLM-24-Jul/.venv/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction = run_example(train_df=dataset.splits[3].data, test_df=dataset.splits[0].data, problem=responses['task_type'], target=responses['target_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f61d093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e45d4f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(prediction, columns\u001b[38;5;241m=\u001b[39m[\u001b[43mdataset_metadata\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      3\u001b[0m result_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_metadata' is not defined"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(prediction, columns=[dataset_metadata[\"target_column\"]])\n",
    "\n",
    "result_df.to_csv(f\"{dataset_path}/predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5e96ab50-b396-4e48-81cc-160058738c4e"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
