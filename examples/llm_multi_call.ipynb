{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e96ab50-b396-4e48-81cc-160058738c4e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ac5964c-acfa-4912-92af-7946c6fa7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from llm_util import run_model_multicall, process_model_responses, save_model_responses\n",
    "from dataset_util import zip_archive, load_dataset_data, print_dataset_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30775d79-68ae-40b3-ab81-ba65f7a224bf",
   "metadata": {},
   "source": [
    "# Загружаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112a810e-b61f-4e94-87c6-766e5a6f80b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4a182b15014f8881be159fba57ba17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "access_token = \"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\"\n",
    "base_model_id = [\n",
    "    \"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "    \"CohereForAI/c4ai-command-r-plus-4bit\"\n",
    "][1]\n",
    "\n",
    "\n",
    "if base_model_id == \"NousResearch/Meta-Llama-3-8B-Instruct\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True, \n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "        low_cpu_mem_usage=True \n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                 quantization_config = bnb_config, \n",
    "                                                 device_map=\"auto\")\n",
    "\n",
    "elif base_model_id == \"CohereForAI/c4ai-command-r-plus-4bit\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=access_token, cache_dir=\"/mnt/disk2/.cache/huggingface/hub\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_id, token=access_token, cache_dir=\"/mnt/disk2/.cache/huggingface/hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cbae526-7666-44fe-ba3b-8c2cfd5e86ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  4 16:45:42 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   43C    P0              64W / 300W |  64790MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1199      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A     20790      C   /home/ubuntu/anaconda3/bin/python         64768MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602b420-fb55-4915-b70d-8e7fd809abcf",
   "metadata": {},
   "source": [
    "# Метаданные датасетов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54a5cd-220e-45ac-9275-67fb2743961d",
   "metadata": {},
   "source": [
    "Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31861f89-efb7-4a7c-a98a-1e5c1c761d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = {\n",
    "    \"name\": \"titanic passengers survival\",\n",
    "    \"description\": \"passengers survived the Titanic shipwreck\",\n",
    "    \"goal\": \"predict which passengers survived the Titanic shipwreck\",\n",
    "    \"split_names\": [\"train\", \"test_X\", \"test_y\"],\n",
    "    \"split_paths\": {\n",
    "        \"train\": 'train.csv',\n",
    "        \"test_X\": 'test.csv',\n",
    "        \"test_y\": 'gender_submission.csv'\n",
    "    },\n",
    "    \"split_descriptions\":{\n",
    "        \"train\": 'train dataset',\n",
    "        \"test_X\": 'test dataset without target',\n",
    "        \"test_y\": 'test dataset target'\n",
    "    },\n",
    "    \"train_split_name\": \"train\"\n",
    "} \n",
    "\n",
    "with open('titanic/metadata.json', 'w') as json_file:\n",
    "    json.dump(dataset_metadata, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2e4a3-6817-43ff-83eb-6ebfc1cbabb0",
   "metadata": {},
   "source": [
    "Credit-G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70078bd5-72cb-4711-9a8a-afb201d11e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = {\n",
    "    \"name\": \"German Credit dataset\",\n",
    "    \"description\": \"classifies people described by a set of attributes as good or bad credit risks\",\n",
    "    \"goal\": \"predict good or bad credit risks of people by attributes\",\n",
    "    \"split_names\": [\"train\", \"test\"],\n",
    "    \"split_paths\": {\n",
    "        \"train\": 'train.csv',\n",
    "        \"test\": 'test.csv',\n",
    "    },\n",
    "    \"split_descriptions\":{\n",
    "        \"train\": 'train dataset',\n",
    "        \"test\": 'test dataset',\n",
    "    },\n",
    "    \"train_split_name\": \"train\"\n",
    "} \n",
    "\n",
    "with open('credit-g/metadata.json', 'w') as json_file:\n",
    "    json.dump(dataset_metadata, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68e62a-66a7-435a-981f-46cc185ca1fa",
   "metadata": {},
   "source": [
    "# Загружаем данные и вызываем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7c01178-2306-48e1-b59b-735fd73ede60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : German Credit dataset\n",
      "description : classifies people described by a set of attributes as good or bad credit risks\n",
      "goal : predict good or bad credit risks of people by attributes\n",
      "split_names : ['train', 'test']\n",
      "split_paths : {'train': 'train.csv', 'test': 'test.csv'}\n",
      "split_descriptions : {'train': 'train dataset', 'test': 'test dataset'}\n",
      "train_split_name : train\n",
      "splits : dict_keys(['train', 'test'])\n"
     ]
    }
   ],
   "source": [
    "dataset_path = ['titanic', 'credit-g'][1]\n",
    "dataset_metadata = load_dataset_data(dataset_path)\n",
    "print_dataset_data(dataset_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "076a8331-4972-4279-9057-aab088a2c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_columns': ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone'], 'target_column': 'class', 'task_type': 'classification'}\n"
     ]
    }
   ],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "if base_model_id == \"CohereForAI/c4ai-command-r-plus-4bit\":\n",
    "    terminators = terminators[0]\n",
    "\n",
    "generation_config = {\n",
    "    \"eos_token_id\": terminators,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 10,\n",
    "}\n",
    "\n",
    "responses = run_model_multicall(model = model,\n",
    "                                tokenizer = tokenizer,\n",
    "                                dataset_metadata = dataset_metadata, \n",
    "                                generation_config = generation_config)\n",
    "\n",
    "responses = process_model_responses(responses)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3629f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_responses(responses, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22a28cce-fc90-4614-8670-28ddf0314c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit-g.zip created successfully.\n"
     ]
    }
   ],
   "source": [
    "zip_archive(dataset_path, f\"{dataset_path}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a3b93-6df8-40fe-b335-1c22fa480a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
