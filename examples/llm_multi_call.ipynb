{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e96ab50-b396-4e48-81cc-160058738c4e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac5964c-acfa-4912-92af-7946c6fa7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from fedot_llm.language_models.llms import HuggingFaceLLM\n",
    "from fedot_llm.language_models.actions import ModelAction\n",
    "from fedot_llm.data.data import Dataset\n",
    "from fedot_llm.fedot_util import run_example\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30775d79-68ae-40b3-ab81-ba65f7a224bf",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5874bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we have a dataset\n",
      "The dataset contains the following splits:\n",
      " \n",
      "The test_merged split stored in file \"test_merged.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Survived']. It is described as None\n",
      "The test split stored in file \"test.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The predictions split stored in file \"predictions.csv\" contains following columns: ['Unnamed: 0', 'Survived']. It is described as None\n",
      "The train split stored in file \"train.csv\" contains following columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The gender_submission split stored in file \"gender_submission.csv\" contains following columns: ['PassengerId', 'Survived']. It is described as None\n",
      "\n",
      "name: None \n",
      "description: None \n",
      "goal: None \n",
      "train_split_name: None \n",
      "splits:\n",
      "\n",
      "name: test_merged \n",
      "path: ../datasets/titanic/test_merged.csv \n",
      "description: None\n",
      "\n",
      "name: test \n",
      "path: ../datasets/titanic/test.csv \n",
      "description: None\n",
      "\n",
      "name: predictions \n",
      "path: ../datasets/titanic/predictions.csv \n",
      "description: None\n",
      "\n",
      "name: train \n",
      "path: ../datasets/titanic/train.csv \n",
      "description: None\n",
      "\n",
      "name: gender_submission \n",
      "path: ../datasets/titanic/gender_submission.csv \n",
      "description: None\n"
     ]
    }
   ],
   "source": [
    "dataset_name = [\n",
    "    'titanic', \n",
    "    'credit-g'\n",
    "][0]\n",
    "dataset_path = os.sep.join(['..', 'datasets', dataset_name])\n",
    "dataset = Dataset.load_from_path(dataset_path)\n",
    "dataset_description = dataset.get_description()\n",
    "dataset_metadata_description = dataset.get_metadata_description()\n",
    "\n",
    "print(dataset_description)\n",
    "print()\n",
    "print(dataset_metadata_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48135276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titanic': 'The sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\nIn this competition, you’ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv.\\n\\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”.\\n\\nThe test.csv dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes.\\n\\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\\n\\nCheck out the “Data” tab to explore the datasets even further. Once you feel you’ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n    \\n    Dataset Description\\nOverview\\nThe data has been split into two groups:\\n\\ntraining set (train.csv)\\ntest set (test.csv)\\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\\n\\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\\n\\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\\n    ',\n",
       " 'credit-g': 'German Credit dataset\\nThis dataset classifies people described by a set of attributes as good or bad credit risks.\\n\\nThis dataset comes with a cost matrix:\\n\\nGood  Bad (predicted)  \\nGood   0    1   (actual)  \\nBad    5    0  \\nIt is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\\n\\nAttribute description\\nStatus of existing checking account, in Deutsche Mark.\\nDuration in months\\nCredit history (credits taken, paid back duly, delays, critical accounts)\\nPurpose of the credit (car, television,...)\\nCredit amount\\nStatus of savings account/bonds, in Deutsche Mark.\\nPresent employment, in number of years.\\nInstallment rate in percentage of disposable income\\nPersonal status (married, single,...) and sex\\nOther debtors / guarantors\\nPresent residence since X years\\nProperty (e.g. real estate)\\nAge in years\\nOther installment plans (banks, stores)\\nHousing (rent, own,...)\\nNumber of existing credits at this bank\\nJob\\nNumber of people being liable to provide maintenance for\\nTelephone (yes,no)\\nForeign worker (yes,no)\\n    ',\n",
       " 'playground-series-s3e23': \"Welcome to the 2023 edition of Kaggle's Playground Series!\\nThank you to everyone who participated in and contributed to Season 3 Playground Series so far!\\n\\nWith the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in October every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\\n\\nYour Goal: Predict defects in C programs given various various attributes about the code.\\n\\nDataset Description\\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\\n\\nFiles\\ntrain.csv - the training dataset; defects is the binary target, which is treated as a boolean (False=0, True=1)\\ntest.csv - the test dataset; your objective is to predict the probability of positive defects (i.e., defects=True)\\nsample_submission.csv - a sample submission file in the correct format\\n\",\n",
       " 'playground-series-s4e3': 'Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting an approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.\\n\\nYour Goal: Predict the probability of various defects on steel plates. Good luck!\\n\\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\\n\\nFiles\\ntrain.csv - the training dataset; there are 7 binary targets: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults\\ntest.csv - the test dataset; your objective is to predict the probability of each of the 7 binary targets\\nsample_submission.csv - a sample submission file in the correct format\\n',\n",
       " 'steel-dataset': 'Content\\nThis company produces several types of coils, steel plates, and iron plates. The information on electricity consumption is held in a cloud-based system. The information on energy consumption of the industry is stored on the website of the Korea Electric Power Corporation (pccs.kepco.go.kr), and the perspectives on daily, monthly, and annual data are calculated and shown.\\n\\nAttribute Information:\\nDate Continuous-time data taken on the first of the month\\nUsage_kWh Industry Energy Consumption Continuous kWh\\nLagging Current reactive power Continuous kVarh\\nLeading Current reactive power Continuous kVarh\\nCO2 Continuous ppm\\nNSM Number of Seconds from midnight Continuous S\\nWeek status Categorical (Weekend (0) or a Weekday(1))\\nDay of week Categorical Sunday, Monday : Saturday\\nLoad Type Categorical Light Load, Medium Load, Maximum Load\\n\\nAcknowledgements\\nThis dataset is sourced from the UCI Machine Learning Repository\\n'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('../datasets/big_descriptions.json', 'r') as json_file:\n",
    "    dataset_big_descriptions = json.load(json_file)\n",
    "dataset_big_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112a810e-b61f-4e94-87c6-766e5a6f80b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4796daef254e5fb01edc0bbe69461a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HuggingFaceLLM(model_id=\"microsoft/Phi-3-mini-4k-instruct\", max_new_tokens=500)\n",
    "action = ModelAction(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602b420-fb55-4915-b70d-8e7fd809abcf",
   "metadata": {},
   "source": [
    "# Уточнение данных о датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d874c889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'Titanic Survival Prediction', 'train_split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "from fedot_llm.language_models import prompts\n",
    "\n",
    "action = ModelAction(model=model)\n",
    "# 1: Название датасета и определение тренировочного сплита\n",
    "task_prompts = {\n",
    "    \"dataset_name\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_name_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"train_split\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.train_split_definition_prompt,\n",
    "        \"context\": dataset.get_description(),\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "operations = {\n",
    "    \"train_split\": lambda x : x.split(\".\")[0]\n",
    "}\n",
    "responses = action.process_model_responses(responses, operations)\n",
    "pprint(responses)\n",
    "\n",
    "dataset.name = responses[\"dataset_name\"]\n",
    "dataset.train_split_name = responses[\"train_split\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221167fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PassengerId': 'the passengerid column represents the unique identifier assigned to each passenger on the titanic, ranging from 1 to 30.',\n",
       " 'Survived': \"the 'survived' column indicates whether a passenger survived (1) or did not survive (0) the titanic disaster.\",\n",
       " 'Pclass': \"the 'pclass' column represents the passenger class on the titanic, with values indicating first class (1), second class (2), and third class (3).\",\n",
       " 'Name': \"the 'name' column in the titanic survival prediction dataset contains the full names of passengers, including their titles and last names, which may provide insights into social status and potential survival rates.\",\n",
       " 'Sex': \"the 'sex' column in the titanic dataset indicates the gender of passengers, with 'male' and 'female' as the possible values.\",\n",
       " 'Age': \"the 'age' column in the titanic survival prediction dataset represents the age of passengers on the titanic, ranging from infants (0.42 years old) to elderly individuals (70.5 years old). the ages are recorded in years, with some values being decimal numbers, indicating fractional years.\",\n",
       " 'SibSp': \"the 'sibsp' column represents the number of siblings or spouses aboard the titanic. it indicates the count of individuals related to the passengers, either as siblings or spouses.\",\n",
       " 'Parch': \"the 'parch' column in the titanic dataset represents the number of parents or siblings aboard the titanic. it indicates the number of immediate family members that a passenger had on board.\",\n",
       " 'Ticket': \"the 'ticket' column in the titanic dataset contains unique identifiers for each passenger's ticket, which may include alphanumeric codes and abbreviations.\",\n",
       " 'Fare': \"the 'fare' column in the titanic survival prediction dataset represents the cost of the ticket for each passenger on the titanic. the values range from as low as 7.25 to as high as 263, indicating a wide range of ticket prices for different classes and accommodations on the ship.\",\n",
       " 'Cabin': \"the 'cabin' column in the titanic dataset represents the cabin number assigned to each passenger on board. the cabin number indicates the passenger's location on the ship, with lower numbers typically located closer to the bow (front) and higher numbers closer to the stern (back). the dataset includes a mix of single and multiple cabin numbers, with some passengers having multiple cabins, as indicated by the 'c23 c25 c27' entry.\",\n",
       " 'Embarked': \"the 'embarked' column indicates the port from which the passengers on the titanic embarked. the values 's' represent southampton, 'c' represent cherbourg, and 'q' represent queenstown.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = list(filter(lambda split: split.name == dataset.train_split_name, dataset.splits))[0]\n",
    "column_descriptions = action.generate_all_column_description(split=train, dataset=dataset)\n",
    "train.set_column_descriptions(column_descriptions)\n",
    "column_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6a54bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_columns': ['Pclass',\n",
      "                         'Sex',\n",
      "                         'Age',\n",
      "                         'SibSp',\n",
      "                         'Parch',\n",
      "                         'Fare',\n",
      "                         'Cabin',\n",
      "                         'Embarked'],\n",
      " 'target_column': 'Survived',\n",
      " 'task_type': 'classification'}\n"
     ]
    }
   ],
   "source": [
    "# 2: Категориальные столбцы, таргет-столбец, тип задачи\n",
    "\n",
    "task_prompts = {\n",
    "    \"categorical_columns\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.categorical_definition_prompt,\n",
    "        \"context\": prompts.categorical_definition_context,\n",
    "    },\n",
    "    \"target_column\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.target_definition_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"task_type\": {\n",
    "        \"system\": dataset_description,\n",
    "        \"task\": prompts.task_definition_prompt,\n",
    "        \"context\": None,\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "\n",
    "pattern = r'[\\'\\\"“”‘’`´]'\n",
    "operations = {\n",
    "    \"categorical_columns\": lambda x : x.split(\"\\n\"),\n",
    "    \"target_column\" :  lambda x : re.sub(pattern, '', x),\n",
    "    \"task_type\": lambda x : re.sub(pattern, '', x.lower())\n",
    "}\n",
    "responses = action.process_model_responses(responses, operations)\n",
    "pprint(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "532a3b93-6df8-40fe-b335-1c22fa480a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24 17:48:31,088 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generations:   0%|          | 0/10000 [00:00<?, ?gen/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24 17:48:42,733 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:48:42,733 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:48:42,733 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:05,287 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:05,292 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:05,356 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:05,754 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:05,883 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:06,095 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:06,125 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:06,188 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-24 17:49:06,244 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generations:   0%|          | 0/10000 [18:16<?, ?gen/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc': 0.941, 'accuracy': 0.828}\n"
     ]
    }
   ],
   "source": [
    "prediction = run_example(train_df=dataset.splits[3].data, test_df=dataset.splits[0].data, problem=responses['task_type'], target=responses['target_column'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
