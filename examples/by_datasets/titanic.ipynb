{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction using FEDOT and LLM\n",
    "\n",
    "This notebook demonstrates the process of analyzing the Titanic dataset and predicting passenger survival using the FEDOT framework enhanced with Large Language Models (LLM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fedot_llm.fedot_util import run_example\n",
    "from fedot_llm.language_models.actions import ModelAction\n",
    "from fedot_llm.language_models.llms import OllamaLLM\n",
    "from fedot_llm.data.data import Dataset\n",
    "from fedot_llm.language_models import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "In this section, we load the Titanic dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'titanic'\n",
    "datasets_folder = os.sep.join([module_path, 'datasets'])\n",
    "dataset_path = os.sep.join([datasets_folder, dataset_name])\n",
    "dataset = Dataset.load_from_path(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The sinking of the Titanic is one of the most infamous shipwrecks in '\n",
      " 'history.\\n'\n",
      " '\\n'\n",
      " 'On April 15, 1912, during her maiden voyage, the widely considered '\n",
      " '“unsinkable” RMS Titanic sank after colliding with an iceberg. '\n",
      " 'Unfortunately, there weren’t enough lifeboats for everyone onboard, '\n",
      " 'resulting in the death of 1502 out of 2224 passengers and crew.\\n'\n",
      " '\\n'\n",
      " 'While there was some element of luck involved in surviving, it seems some '\n",
      " 'groups of people were more likely to survive than others.\\n'\n",
      " '\\n'\n",
      " 'In this challenge, we ask you to build a predictive model that answers the '\n",
      " 'question: “what sorts of people were more likely to survive?” using '\n",
      " 'passenger data (ie name, age, gender, socio-economic class, etc).\\n'\n",
      " '\\n'\n",
      " 'In this competition, you’ll gain access to two similar datasets that include '\n",
      " 'passenger information like name, age, gender, socio-economic class, etc. One '\n",
      " 'dataset is titled train.csv and the other is titled test.csv.\\n'\n",
      " '\\n'\n",
      " 'Train.csv will contain the details of a subset of the passengers on board '\n",
      " '(891 to be exact) and importantly, will reveal whether they survived or not, '\n",
      " 'also known as the “ground truth”.\\n'\n",
      " '\\n'\n",
      " 'The test.csv dataset contains similar information but does not disclose the '\n",
      " '“ground truth” for each passenger. It’s your job to predict these outcomes.\\n'\n",
      " '\\n'\n",
      " 'Using the patterns you find in the train.csv data, predict whether the other '\n",
      " '418 passengers on board (found in test.csv) survived.\\n'\n",
      " '\\n'\n",
      " 'Check out the “Data” tab to explore the datasets even further. Once you feel '\n",
      " 'you’ve created a competitive model, submit it to Kaggle to see where your '\n",
      " 'model stands on our leaderboard against other Kagglers.\\n'\n",
      " '    \\n'\n",
      " '    Dataset Description\\n'\n",
      " 'Overview\\n'\n",
      " 'The data has been split into two groups:\\n'\n",
      " '\\n'\n",
      " 'training set (train.csv)\\n'\n",
      " 'test set (test.csv)\\n'\n",
      " 'The training set should be used to build your machine learning models. For '\n",
      " 'the training set, we provide the outcome (also known as the “ground truth”) '\n",
      " 'for each passenger. Your model will be based on “features” like passengers’ '\n",
      " 'gender and class. You can also use feature engineering to create new '\n",
      " 'features.\\n'\n",
      " '\\n'\n",
      " 'The test set should be used to see how well your model performs on unseen '\n",
      " 'data. For the test set, we do not provide the ground truth for each '\n",
      " 'passenger. It is your job to predict these outcomes. For each passenger in '\n",
      " 'the test set, use the model you trained to predict whether or not they '\n",
      " 'survived the sinking of the Titanic.\\n'\n",
      " '\\n'\n",
      " 'We also include gender_submission.csv, a set of predictions that assume all '\n",
      " 'and only female passengers survive, as an example of what a submission file '\n",
      " 'should look like.\\n')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "description_file = 'big_descriptions.json'\n",
    "with open(os.sep.join([datasets_folder, description_file]), 'r') as json_file:\n",
    "    dataset_big_descriptions = json.load(json_file)\n",
    "big_description = dataset_big_descriptions[dataset_name]\n",
    "pprint(big_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we have a datasetThe dataset contains the following splits:\n",
      "\n",
      "The test_merged split stored in file \"test_merged.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Survived']. It is described as None\n",
      "The test split stored in file \"test.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The predictions split stored in file \"predictions.csv\" contains following columns: ['Unnamed: 0', 'Survived']. It is described as None\n",
      "The train split stored in file \"train.csv\" contains following columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The gender_submission split stored in file \"gender_submission.csv\" contains following columns: ['PassengerId', 'Survived']. It is described as None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "splits:\n",
      "\n",
      "name: test_merged \n",
      "path: /Users/aleksejlapin/Work/AutoML-LLM/AutoML-LLM-24-Jul/datasets/titanic/test_merged.csv \n",
      "description: None\n",
      "\n",
      "name: test \n",
      "path: /Users/aleksejlapin/Work/AutoML-LLM/AutoML-LLM-24-Jul/datasets/titanic/test.csv \n",
      "description: None\n",
      "\n",
      "name: predictions \n",
      "path: /Users/aleksejlapin/Work/AutoML-LLM/AutoML-LLM-24-Jul/datasets/titanic/predictions.csv \n",
      "description: None\n",
      "\n",
      "name: train \n",
      "path: /Users/aleksejlapin/Work/AutoML-LLM/AutoML-LLM-24-Jul/datasets/titanic/train.csv \n",
      "description: None\n",
      "\n",
      "name: gender_submission \n",
      "path: /Users/aleksejlapin/Work/AutoML-LLM/AutoML-LLM-24-Jul/datasets/titanic/gender_submission.csv \n",
      "description: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.detailed_description)\n",
    "print(\"-\"*100)\n",
    "print(dataset.metadata_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis using LLM\n",
    "\n",
    "Here we use LLM to analyze and describe various aspects of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model='llama3')\n",
    "action = ModelAction(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset Name, Description and Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: Titanic Passenger Survival Dataset\n",
      "Dataset description: Here is a short description of the dataset:\n",
      "\n",
      "This dataset contains information about passengers on board the RMS Titanic, including passenger IDs, classes, names, genders, ages, and other relevant details. The data is split into four files: \"train.csv\" for training, \"test.csv\" and \"test_merged.csv\" for testing, and \"predictions.csv\" for submitting predicted survival outcomes. The goal is to build a predictive model that answers the question \"what sorts of people were more likely to survive?\" using passenger information from the train dataset and predict the survival outcomes for the test passengers based on patterns learned from the training data.\n",
      "Dataset goal: Predict whether or not each passenger survived the sinking of the Titanic based on their demographic information and other characteristics. The target column is \"Survived\".\n"
     ]
    }
   ],
   "source": [
    "task_prompts = {\n",
    "    \"dataset_name\": {\n",
    "        \"system\": big_description,\n",
    "        \"task\": prompts.dataset_name_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"dataset_description\": {\n",
    "        \"system\": big_description,\n",
    "        \"task\": prompts.dataset_description_prompt,\n",
    "        \"context\": dataset.detailed_description,\n",
    "    },\n",
    "    \"dataset_goal\": {\n",
    "        \"system\": big_description,\n",
    "        \"task\": prompts.dataset_goal_prompt,\n",
    "        \"context\": dataset.description,\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "dataset.name = responses[\"dataset_name\"]\n",
    "dataset.description = responses[\"dataset_description\"]\n",
    "dataset.goal = responses[\"dataset_goal\"]\n",
    "\n",
    "print(f\"Dataset name: {dataset.name}\")\n",
    "print(f\"Dataset description: {dataset.description}\")\n",
    "print(f\"Dataset goal: {dataset.goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we have a dataset called Titanic Passenger Survival Dataset\n",
      "It could be described as following: Here is a short description of the dataset:\n",
      "\n",
      "This dataset contains information about passengers on board the RMS Titanic, including passenger IDs, classes, names, genders, ages, and other relevant details. The data is split into four files: \"train.csv\" for training, \"test.csv\" and \"test_merged.csv\" for testing, and \"predictions.csv\" for submitting predicted survival outcomes. The goal is to build a predictive model that answers the question \"what sorts of people were more likely to survive?\" using passenger information from the train dataset and predict the survival outcomes for the test passengers based on patterns learned from the training data.\n",
      "The goal is: Predict whether or not each passenger survived the sinking of the Titanic based on their demographic information and other characteristics. The target column is \"Survived\".\n",
      "The dataset contains the following splits:\n",
      "\n",
      "The test_merged split stored in file \"test_merged.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Survived']. It is described as None\n",
      "The test split stored in file \"test.csv\" contains following columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The predictions split stored in file \"predictions.csv\" contains following columns: ['Unnamed: 0', 'Survived']. It is described as None\n",
      "The train split stored in file \"train.csv\" contains following columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']. It is described as None\n",
      "The gender_submission split stored in file \"gender_submission.csv\" contains following columns: ['PassengerId', 'Survived']. It is described as None\n"
     ]
    }
   ],
   "source": [
    "print(dataset.detailed_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset Train and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "name: train \n",
      "path: /Users/aleksejlapin/Work/AutoML-LLM/AutoML-LLM-24-Jul/datasets/titanic/train.csv \n",
      "description: None\n",
      "\n",
      "\n",
      "Test split:\n",
      "name: test \n",
      "path: /Users/aleksejlapin/Work/AutoML-LLM/AutoML-LLM-24-Jul/datasets/titanic/test.csv \n",
      "description: None\n"
     ]
    }
   ],
   "source": [
    "task_prompts = {\n",
    "    \"train_split\": {\n",
    "        \"system\": dataset.detailed_description,\n",
    "        \"task\": prompts.train_split_definition_prompt,\n",
    "        \"context\": f\"Available splits:\\n{dataset.metadata_description}\",\n",
    "    },\n",
    "    \"test_split\": {\n",
    "        \"system\": dataset.detailed_description,\n",
    "        \"task\": prompts.test_split_definition_prompt,\n",
    "        \"context\": f\"Available splits:\\n{dataset.metadata_description}\",\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "operations = {\n",
    "    \"train_split\": lambda x : x.split(\".\")[0],\n",
    "    \"test_split\": lambda x  : x.split(\".\")[0],\n",
    "}\n",
    "responses = action.process_model_responses(responses, operations)\n",
    "\n",
    "dataset.train_split = responses[\"train_split\"]\n",
    "dataset.test_split = responses[\"test_split\"]\n",
    "\n",
    "\n",
    "print(f\"Train split:\\n{dataset.train_split}\\n\\n\")\n",
    "print(f\"Test split:\\n{dataset.test_split}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': 'The age of a passenger, ranging from children (0-5 years) to adults '\n",
      "        'and seniors, with some missing values (nan).',\n",
      " 'Cabin': 'The cabin numbers range from single letters indicating cabins '\n",
      "          'without numbers, to combinations of letters and numbers '\n",
      "          'representing specific cabins.',\n",
      " 'Embarked': 'The embarked location of the passengers, with values '\n",
      "             'representing Southampton (S), Cherbourg (C), Queenstown (Q), and '\n",
      "             'unknown (nan)',\n",
      " 'Fare': 'Passenger fares, ranging from approximately $7 to over $500.',\n",
      " 'Name': \"Passenger names in the format of 'Last Name', Mr./Mrs./Miss.\",\n",
      " 'Parch': 'Number of parents or children aboard, with possible values being '\n",
      "          'the number of immediate family members (0-5)',\n",
      " 'PassengerId': 'Unique identifier for each passenger.',\n",
      " 'Pclass': 'Passenger class, with 1st class being the most luxurious and 3rd '\n",
      "           'class being the least.',\n",
      " 'Sex': 'Indicates whether a passenger is male or female.',\n",
      " 'SibSp': 'Number of siblings or spouses on board, ranging from 0 to 8',\n",
      " 'Survived': 'Indicator of whether a passenger survived (0 = No, 1 = Yes)',\n",
      " 'Ticket': 'Unique identification numbers and codes assigned to each passenger '\n",
      "           'on the Titanic.'}\n"
     ]
    }
   ],
   "source": [
    "column_descriptions = action.generate_all_column_description(split=dataset.train_split, dataset=dataset)\n",
    "dataset.train_split.set_column_descriptions(column_descriptions)\n",
    "pprint(column_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Column and Task Type Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column: Survived\n",
      "Task type: classification\n"
     ]
    }
   ],
   "source": [
    "task_prompts = {\n",
    "    \"target_column\": {\n",
    "        \"system\": dataset.description,\n",
    "        \"task\": prompts.target_definition_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"task_type\": {\n",
    "        \"system\": dataset.description,\n",
    "        \"task\": prompts.task_definition_prompt,\n",
    "        \"context\": None,\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "\n",
    "pattern = r'[\\'\\\"“”‘’`´]'\n",
    "operations = {\n",
    "    \"target_column\" :  lambda x : re.sub(pattern, '', x),\n",
    "    \"task_type\": lambda x : re.sub(pattern, '', x.lower())\n",
    "}\n",
    "responses = action.process_model_responses(responses, operations)\n",
    "dataset.target_name = responses[\"target_column\"]\n",
    "dataset.task_type = responses[\"task_type\"]\n",
    "\n",
    "print(f\"Target column: {dataset.target_name}\")\n",
    "print(f\"Task type: {dataset.task_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Columns Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Survived', 'Pclass', 'Name', 'Sex', 'Parch', 'Ticket', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "categorical_columns =  action.get_categorical_features(split=dataset.train_split, dataset=dataset)\n",
    "pprint(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEDOT Framework Execution\n",
    "In this section, we prepare the data and run the FEDOT framework to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-29 17:25:24,519 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generations:   0%|          | 0/10000 [00:00<?, ?gen/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-29 17:25:30,884 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:30,884 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:30,884 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,481 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,481 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,483 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,484 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,484 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,489 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,492 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,501 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n",
      "2024-07-29 17:25:52,502 - Topological features operation requires extra dependencies for time series forecasting, which are not installed. It can infuence the performance. Please install it by 'pip install fedot[extra]'\n"
     ]
    }
   ],
   "source": [
    "prediction = run_example(train_df=dataset.train_split.data, test_df=dataset.test_split.data, problem=dataset.task_type, target=dataset.target_name, timeout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here we display and analyze the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
