{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction using FEDOT and LLM\n",
    "\n",
    "This notebook demonstrates the process of analyzing the Titanic dataset and predicting passenger survival using the FEDOT framework enhanced with Large Language Models (LLM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fedot_llm.data.zip import unzip_archive\n",
    "from fedot_llm.fedot_util import run_example\n",
    "from fedot_llm.language_models.actions import ModelAction\n",
    "from fedot_llm.language_models.llms import OllamaLLM\n",
    "from fedot_llm.data.data import Dataset\n",
    "import fedot_llm.language_models.prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "In this section, we load the Titanic dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'titanic'\n",
    "dataset_path = os.sep.join(['..', 'datasets', dataset_name])\n",
    "dataset = Dataset.load_from_path(dataset_path)\n",
    "dataset_description = dataset.description\n",
    "dataset_metadata_description = dataset.metadata_description\n",
    "\n",
    "print(dataset_description)\n",
    "print()\n",
    "print(dataset_metadata_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../datasets/big_descriptions.json', 'r') as json_file:\n",
    "    dataset_big_descriptions = json.load(json_file)\n",
    "dataset_big_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(filter(lambda split: split.name == dataset.train_split_name, dataset.splits))[0]\n",
    "train.data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(filter(lambda split: split.name == 'test_merged', dataset.splits))[0]\n",
    "test.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis using LLM\n",
    "\n",
    "Here we use LLM to analyze and describe various aspects of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedot_llm.language_models import prompts\n",
    "\n",
    "action = ModelAction(model=model)\n",
    "# 1: Название датасета и определение тренировочного сплита\n",
    "task_prompts = {\n",
    "    \"dataset_name\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_name_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"train_split\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.train_split_definition_prompt,\n",
    "        \"context\": dataset.description,\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "operations = {\n",
    "    \"train_split\": lambda x : x.split(\".\")[0]\n",
    "}\n",
    "responses = action.process_model_responses(responses, operations)\n",
    "pprint(responses)\n",
    "\n",
    "dataset.name = responses[\"dataset_name\"]\n",
    "dataset.train_split_name = responses[\"train_split\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description and Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompts = {\n",
    "    \"dataset_description\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_description_prompt,\n",
    "        \"context\": dataset.description,\n",
    "    },\n",
    "    \"dataset_goal\": {\n",
    "        \"system\": dataset_big_descriptions[dataset_name],\n",
    "        \"task\": prompts.dataset_goal_prompt,\n",
    "        \"context\": dataset.description,\n",
    "    },\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "pprint(responses)\n",
    "\n",
    "dataset.description = responses[\"dataset_description\"]\n",
    "dataset.goal = responses[\"dataset_goal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_descriptions = action.generate_all_column_description(split=train, dataset=dataset)\n",
    "train.set_column_descriptions(column_descriptions)\n",
    "column_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Column and Task Type Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompts = {\n",
    "    \"target_column\": {\n",
    "        \"system\": dataset.description,\n",
    "        \"task\": prompts.target_definition_prompt,\n",
    "        \"context\": None,\n",
    "    },\n",
    "    \"task_type\": {\n",
    "        \"system\": dataset.description,\n",
    "        \"task\": prompts.task_definition_prompt,\n",
    "        \"context\": None,\n",
    "    }\n",
    "}\n",
    "\n",
    "responses = action.run_model_multicall(\n",
    "    task_prompts\n",
    ")\n",
    "\n",
    "pattern = r'[\\'\\\"“”‘’`´]'\n",
    "operations = {\n",
    "    \"target_column\" :  lambda x : re.sub(pattern, '', x),\n",
    "    \"task_type\": lambda x : re.sub(pattern, '', x.lower())\n",
    "}\n",
    "responses = action.process_model_responses(responses, operations)\n",
    "pprint(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Columns Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns =  action.get_categorical_features(split=train, dataset=dataset)\n",
    "pprint(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEDOT Framework Execution\n",
    "In this section, we prepare the data and run the FEDOT framework to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = run_example(train_df=train.data, test_df=test.data, problem=responses['task_type'], target=responses['target_column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here we display and analyze the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[:5]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
